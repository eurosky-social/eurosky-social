alertmanager:
  enabled: true
  ingress:
    enabled: false # TODO: Enable with TLS, IP whitelisting, external auth (NEVER expose without protection)
  alertmanagerSpec:
    replicas: 1 # TODO: Increase to 2+ for HA (auto-creates gossip cluster for alert deduplication)
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: ${storage_class}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 2Gi # TODO: Increase to 10Gi for production notification/silence history
    resources: # TODO: Scale up based on actual usage metrics
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        memory: 128Mi
    podAntiAffinity: hard
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway # TODO: Change to DoNotSchedule for strict multi-AZ in production
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: alertmanager
    # TODO: Configure alert routing, notification channels (PagerDuty/Slack), escalation policies

grafana:
  enabled: true
  adminPassword: ${grafana_admin_password} # TODO: Configure SSO/OAuth integration for production
  replicas: 1 # TODO: Increase to 2+ for HA
  ingress:
    enabled: false # TODO: Enable with TLS, IP whitelisting, external auth
  persistence:
    enabled: true
    storageClassName: ${storage_class}
    size: 2Gi # TODO: Increase to 10Gi for production dashboard persistence
  resources: # TODO: Scale up based on actual usage
    requests:
      cpu: 10m
      memory: 64Mi
    limits:
      memory: 256Mi
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: grafana
          topologyKey: kubernetes.io/hostname
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway # TODO: Change to DoNotSchedule for strict multi-AZ in production
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: grafana
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      node-exporter:
        gnetId: 1860
        revision: 31
        datasource: Prometheus
      pod-metrics:
        gnetId: 6417
        revision: 1
        datasource: Prometheus

prometheus:
  enabled: true
  ingress:
    enabled: false # TODO: Enable with TLS, IP whitelisting, external auth + set externalUrl in prometheusSpec
  prometheusSpec:
    replicas: 1 # TODO: Increase to 2+ for HA (consider functional sharding for large deployments)
    retention: 7d # TODO: Increase to 30d for production
    retentionSize: "8GB" # TODO: Adjust based on actual TSDB usage
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ${storage_class} # TODO: Use high I/O SSD volumes for production
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi # TODO: Increase to 50Gi+ for production (with backup/snapshot policies)
    resources: # TODO: Scale up based on TSDB metrics and memory pressure
      requests:
        cpu: 50m
        memory: 256Mi
      limits:
        memory: 1Gi
    podAntiAffinity: hard
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway # TODO: Change to DoNotSchedule for strict multi-AZ in production
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus
    # TODO: Add sessionAffinity on Service or deploy Thanos Querier for consistent query results across replicas

prometheus-node-exporter:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 16Mi
    limits:
      memory: 32Mi

kube-state-metrics:
  enabled: true
  replicas: 1 # TODO: Run multiple replicas with load balancing for HA
  resources:
    requests:
      cpu: 10m
      memory: 16Mi
    limits:
      memory: 64Mi

prometheusOperator:
  enabled: true
  replicas: 1 # Single pod only - potential SPOF for rule deployment
  resources:
    requests:
      cpu: 10m
      memory: 32Mi
    limits:
      memory: 128Mi
  # TODO: For private GKE clusters, add firewall rules for control plane -> webhook pods access
  # TODO: Or disable admission webhooks via admissionWebhooks.enabled=false
