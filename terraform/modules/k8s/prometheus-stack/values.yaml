alertmanager:
  enabled: true
  ingress:
    enabled: false # TODO: Enable with TLS, IP whitelisting, external auth (NEVER expose without protection)
  alertmanagerSpec:
    replicas: 1 # TODO: Increase to 2+ for HA (auto-creates gossip cluster for alert deduplication)
    storage:
      volumeClaimTemplate:
        spec:
          storageClassName: ${storage_class}
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 2Gi # TODO: Increase to 10Gi for production notification/silence history
    resources: # TODO: Scale up based on actual usage metrics
      requests:
        cpu: 10m
        memory: 32Mi
      limits:
        memory: 128Mi
    podAntiAffinity: hard
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway # TODO: Change to DoNotSchedule for strict multi-AZ in production
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: alertmanager
    # TODO: Configure alert routing, notification channels (PagerDuty/Slack), escalation policies

grafana:
  enabled: true
  adminPassword: ${grafana_admin_password} # TODO: Configure SSO/OAuth integration for production
  replicas: 1 # TODO: Increase to 2+ for HA
  ingress:
    enabled: true
    ingressClassName: nginx
    annotations:
      cert-manager.io/cluster-issuer: letsencrypt-prod
      external-dns.alpha.kubernetes.io/target: ingress.${cluster_domain}
      # TODO: Add Tailscale access with IP whitelisting (ingress.kubernetes.io/whitelist-source-range: <tailscale-network-cidr>)
      # TODO: SECURITY WARNING - Currently exposed without IP whitelisting or external auth!
    hosts:
      - grafana.${cluster_domain}
    path: /
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.${cluster_domain}
  persistence:
    enabled: true
    storageClassName: ${storage_class}
    size: 2Gi # TODO: Increase to 10Gi for production dashboard persistence
  resources: # TODO: Scale up based on actual usage
    requests:
      cpu: 50m
      memory: 256Mi
    limits:
      memory: 512Mi
  topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: topology.kubernetes.io/zone
      whenUnsatisfiable: ScheduleAnyway # TODO: Change to DoNotSchedule for strict multi-AZ in production
      labelSelector:
        matchLabels:
          app.kubernetes.io/name: grafana
  additionalDataSources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki.monitoring.svc.cluster.local:3100
      jsonData:
        maxLines: 1000
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ""
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
  dashboards:
    default:
      # Kubernetes Overview Dashboards
      kubernetes-cluster:
        gnetId: 7249
        revision: 1
        datasource: Prometheus
      kubernetes-global:
        gnetId: 15760
        revision: 1
        datasource: Prometheus
      kubernetes-namespaces:
        gnetId: 15757
        revision: 1
        datasource: Prometheus
      kubernetes-pods:
        gnetId: 15758
        revision: 1
        datasource: Prometheus

      # Node and Host Metrics
      node-exporter:
        gnetId: 1860
        revision: 31
        datasource: Prometheus
      pod-metrics:
        gnetId: 6417
        revision: 1
        datasource: Prometheus

      # Ingress-Nginx
      nginx-ingress:
        gnetId: 9614
        revision: 1
        datasource: Prometheus

      # PostgreSQL (CloudNativePG)
      cloudnativepg:
        gnetId: 15915
        revision: 1
        datasource: Prometheus
      postgresql-database:
        gnetId: 9628
        revision: 7
        datasource: Prometheus

      # cert-manager
      cert-manager:
        gnetId: 11001
        revision: 1
        datasource: Prometheus

      # Prometheus & Alertmanager
      prometheus-stats:
        gnetId: 3662
        revision: 2
        datasource: Prometheus
      alertmanager:
        gnetId: 19268
        revision: 1
        datasource: Prometheus

      # Loki (Log Aggregation)
      loki-logs:
        gnetId: 13639
        revision: 2
        datasource: Loki

prometheus:
  enabled: true
  ingress:
    enabled: false # TODO: Enable with TLS, IP whitelisting, external auth + set externalUrl in prometheusSpec
  prometheusSpec:
    replicas: 1 # TODO: Increase to 2+ for HA (consider functional sharding for large deployments)
    retention: 7d # TODO: Increase to 30d for production
    retentionSize: "8GB" # TODO: Adjust based on actual TSDB usage
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ${storage_class} # TODO: Use high I/O SSD volumes for production
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi # TODO: Increase to 50Gi+ for production (with backup/snapshot policies)
    resources: # TODO: Scale up based on TSDB metrics and memory pressure
      requests:
        cpu: 50m
        memory: 256Mi
      limits:
        memory: 1Gi
    podAntiAffinity: hard
    topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway # TODO: Change to DoNotSchedule for strict multi-AZ in production
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus
    # TODO: Add sessionAffinity on Service or deploy Thanos Querier for consistent query results across replicas

prometheus-node-exporter:
  enabled: true
  resources:
    requests:
      cpu: 10m
      memory: 16Mi
    limits:
      memory: 32Mi

kube-state-metrics:
  enabled: true
  replicas: 1 # TODO: Run multiple replicas with load balancing for HA
  resources:
    requests:
      cpu: 10m
      memory: 16Mi
    limits:
      memory: 64Mi

prometheusOperator:
  enabled: true
  replicas: 1 # Single pod only - potential SPOF for rule deployment
  resources:
    requests:
      cpu: 10m
      memory: 32Mi
    limits:
      memory: 128Mi
  # TODO: For private GKE clusters, add firewall rules for control plane -> webhook pods access
  # TODO: Or disable admission webhooks via admissionWebhooks.enabled=false
